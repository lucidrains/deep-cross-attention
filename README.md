## Deep Cross Attention (wip)

Implementation of the proposed [DeepCrossAttention](https://arxiv.org/abs/2502.06785) by Heddes et al while at Google research, in Pytorch

## Citations

```bibtex
@inproceedings{Heddes2025DeepCrossAttentionST,
    title   = {DeepCrossAttention: Supercharging Transformer Residual Connections},
    author  = {Mike Heddes and Adel Javanmard and Kyriakos Axiotis and Gang Fu and MohammadHossein Bateni and Vahab S. Mirrokni},
    year    = {2025},
    url     = {https://api.semanticscholar.org/CorpusID:276250576}
}
```
